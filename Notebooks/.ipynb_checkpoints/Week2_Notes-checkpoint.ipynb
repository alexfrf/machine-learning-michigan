{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Machine Learning: Module 2 (Supervised Learning, Part I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_regression\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler #importing the scaler method\n",
    "from sklearn.datasets import make_friedman1\n",
    "\n",
    "base_dir = os.path.join('',Path(os.getcwd()).parents[0])\n",
    "data_dir = os.path.join(base_dir, 'Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preamble: The K-NN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'fruit_data_with_colors.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23436/3145379179.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfruits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fruit_data_with_colors.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfruits_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'mass'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'width'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'height'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'color_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX_fruits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfruits\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfruits_columns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_fruits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfruits\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'fruit_label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtarget_names_fruits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfruits\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'fruit_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_table\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    777\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1218\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    787\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    790\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fruit_data_with_colors.txt'"
     ]
    }
   ],
   "source": [
    "fruits = pd.read_table('fruit_data_with_colors.txt')\n",
    "fruits_columns = ['mass', 'width', 'height','color_score']\n",
    "X_fruits = fruits[fruits_columns]\n",
    "y_fruits = fruits['fruit_label']\n",
    "target_names_fruits = list(fruits['fruit_name'].unique())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_fruits, y_fruits, random_state=0)\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "knn.fit(X_train, y_train)\n",
    "print('Accuracy of K-NN classifier on training set: {:.2f}'\n",
    "     .format(knn.score(X_train, y_train)))\n",
    "print('Accuracy of K-NN classifier on test set: {:.2f}'\n",
    "     .format(knn.score(X_test, y_test)))\n",
    "\n",
    "example_fruit = [[5.5,2.2,10,0.7]]\n",
    "example_fruit_scaled = scaler.transform(example_fruit)\n",
    "print('Predicted fruit type for ', example_fruit, ' is ', \n",
    "          target_names_fruits[knn.predict(example_fruit)[0]-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "# we must apply the scaling to the test set that we computed for the training set\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "print('Accuracy of K-NN classifier on training set: {:.2f}'\n",
    "     .format(knn.score(X_train_scaled, y_train)))\n",
    "print('Accuracy of K-NN classifier on test set: {:.2f}'\n",
    "     .format(knn.score(X_test_scaled, y_test)))\n",
    "\n",
    "example_fruit = [[5.5, 2.2, 10, 0.70]]\n",
    "example_fruit_scaled = scaler.transform(example_fruit)\n",
    "print('Predicted fruit type for ', example_fruit, ' is ', \n",
    "          target_names_fruits[knn.predict(example_fruit_scaled)[0]-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple Regression Dataset - make_regression()** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic dataset for simple regression\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Sample regression problem with one input variable')\n",
    "X_R1, y_R1 = make_regression (n_samples = 100, n_features = 1, n_informative =1, bias=150.0, noise = 30, random_state =0) # random regression, given parameters\n",
    "plt.scatter(X_R1,y_R1, marker = 'o', s=50)\n",
    "\n",
    "data= pd.DataFrame(data = X_R1, columns =['x'])\n",
    "data ['y'] = pd.DataFrame(data = y_R1, columns =['y'])\n",
    "b, a, r_value, p_value, std_err = stats.linregress(x=data['x'],y=data['y'])\n",
    "line_reg = [b*i + a for i in data.x]\n",
    "reg = 'y = {}x + {}'\n",
    "\n",
    "plt.plot(data.x,line_reg, 'r', label=reg.format(b.round(2),a.round(2)))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#or\n",
    "sns.lmplot('x', 'y', data)\n",
    "sns.despine()\n",
    "plt.annotate(reg.format(b.round(2),a.round(2)),(-2,240))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Polynomial Regression Dataset - make_friedman1()** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic dataset for more complex regression\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Complex regression problem with one input variable')\n",
    "X_F1, y_F1 = make_friedman1(n_samples = 100,\n",
    "                           n_features = 7, random_state=0)\n",
    "\n",
    "plt.scatter(X_F1[:, 2], y_F1, marker= 'o', s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary classification Dataset - make_classification()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from matplotlib.colors import ListedColormap\n",
    "cmap_bold = ListedColormap(['#FFFF00', '#00FF00', '#0000FF','#000000'])\n",
    "plt.figure()\n",
    "plt.title('Sample binary classification problem with two informative features')\n",
    "X_C2, y_C2 = make_classification(n_samples = 100, n_features=2,\n",
    "                                n_redundant=0, n_informative=2,\n",
    "                                n_clusters_per_class=1, flip_y = 0.1,\n",
    "                                class_sep = 0.5, random_state=0)\n",
    "plt.scatter(X_C2[:, 0], X_C2[:, -1], c=y_C2,\n",
    "           marker= 'o', s=50, cmap=cmap_bold)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_C2[:5])\n",
    "print(y_C2[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Complex Binary classification Dataset - make_blobs()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X_D2, y_D2 = make_blobs(n_samples = 100, n_features = 2, centers = 8,\n",
    "                       cluster_std = 1.3, random_state = 4)\n",
    "y_D2 = y_D2 % 2\n",
    "plt.figure()\n",
    "plt.title('Sample binary classification problem with non-linearly separable classes')\n",
    "plt.scatter(X_D2[:,0], X_D2[:,1], c=y_D2,\n",
    "           marker= 'o', s=50, cmap=cmap_bold)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing some datasets for future work**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "print(cancer.keys())\n",
    "(X_cancer, y_cancer) = load_breast_cancer(return_X_y = True)\n",
    "X_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adspy_shared_utilities import load_crime_dataset\n",
    "\n",
    "(X_crime, y_crime) = load_crime_dataset()\n",
    "X_crime.communityname.unique().shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the accuracy and suitability of the classifier depending on the k for both train and test datasets at the same time. We'll evaluate the score of the classifier for both datasets in order to figure out the most suitable k for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from adspy_shared_utilities import plot_two_class_knn\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2,random_state=0)\n",
    "\n",
    "plot_two_class_knn(X_train, y_train, 1, 'uniform', X_test, y_test) # k=1\n",
    "plot_two_class_knn(X_train, y_train, 3, 'uniform', X_test, y_test) # k=3\n",
    "plot_two_class_knn(X_train, y_train, 11, 'uniform', X_test, y_test) # k=11\n",
    "plot_two_class_knn(X_train, y_train, 20, 'uniform', X_test, y_test) # k=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In the k = 1 case, the training score is a perfect 1.0. But the test score is only 0.80. As k increases to 3, the training score drops to 0.88 but the test score rises slightly 2.88, indicating the model is generalizing better to new data. When k = 11, the training score drops a bit further to 0.81, but the test score even better at 0.92, indicating that this simple model is much more effective at ignoring minor variations in training data. And instead capturing the more important global trend in where the classes tend to be located with the best overall generalization performance as a result*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Regression - KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_R1, y_R1, random_state = 0)\n",
    "knnreg = KNeighborsRegressor(n_neighbors = 5)\n",
    "knnreg = knnreg.fit(X_train, y_train)\n",
    "print(knnreg.predict(X_test))\n",
    "print('R-squared test score: {:.3f}'\n",
    "     .format(knnreg.score(X_test, y_test)))\n",
    "print('R-squared train score: {:.3f}'\n",
    "     .format(knnreg.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knnreg5 = KNeighborsRegressor(n_neighbors = 5).fit(X_train, y_train)\n",
    "print('R-squared test score for k=5: {:.3f}'\n",
    "     .format(knnreg5.score(X_test, y_test)))\n",
    "\n",
    "knnreg1 = KNeighborsRegressor(n_neighbors = 1).fit(X_train, y_train)\n",
    "print('R-squared test score for k=1: {:.3f}'\n",
    "     .format(knnreg1.score(X_test, y_test)))\n",
    "\n",
    "knnreg15 = KNeighborsRegressor(n_neighbors = 15).fit(X_train, y_train)\n",
    "print('R-squared test score for k=15: {:.3f}'\n",
    "     .format(knnreg15.score(X_test, y_test)))\n",
    "\n",
    "knnreg55 = KNeighborsRegressor(n_neighbors = 55).fit(X_train, y_train)\n",
    "print('R-squared test score for k=55: {:.3f}'\n",
    "     .format(knnreg55.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R-Squared test score (coef of determination)** sets how well the regression model is able to fit to real data. In this case, with k=5 is RSQ=0.425. The nearer to 1 the better the prediction would be. It will be better for k=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the accuracy of the prediction in training sets with k=1 and k=3\n",
    "\n",
    "fig, subaxes = plt.subplots(1, 2, figsize=(8,4))\n",
    "X_predict_input = np.linspace(-3, 3, 50).reshape(-1,1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_R1[0::5], y_R1[0::5], random_state = 0)\n",
    "\n",
    "for thisaxis, K in zip(subaxes, [1, 3]):\n",
    "    knnreg = KNeighborsRegressor(n_neighbors = K).fit(X_train, y_train)\n",
    "    y_predict_output = knnreg.predict(X_predict_input)\n",
    "    thisaxis.set_xlim([-2.5, 0.75])\n",
    "    thisaxis.plot(X_predict_input, y_predict_output, '^', markersize = 10,\n",
    "                 label='Predicted', alpha=0.8)\n",
    "    thisaxis.plot(X_train, y_train, 'o', label='True Value', alpha=0.8)\n",
    "    thisaxis.set_xlabel('Input feature')\n",
    "    thisaxis.set_ylabel('Target value')\n",
    "    thisaxis.set_title('KNN regression (K={})'.format(K))\n",
    "    thisaxis.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Regression model complexity as a function of K **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot k-NN regression on sample dataset for different values of K\n",
    "fig, subaxes = plt.subplots(5, 1, figsize=(5,20))\n",
    "X_predict_input = np.linspace(-3, 3, 500).reshape(-1,1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_R1, y_R1,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "for thisaxis, K in zip(subaxes, [1, 3, 7, 15, 55]):\n",
    "    knnreg = KNeighborsRegressor(n_neighbors = K).fit(X_train, y_train)\n",
    "    y_predict_output = knnreg.predict(X_predict_input)\n",
    "    train_score = knnreg.score(X_train, y_train)\n",
    "    test_score = knnreg.score(X_test, y_test)\n",
    "    thisaxis.plot(X_predict_input, y_predict_output)\n",
    "    thisaxis.plot(X_train, y_train, 'o', alpha=0.9, label='Train')\n",
    "    thisaxis.plot(X_test, y_test, '^', alpha=0.9, label='Test')\n",
    "    thisaxis.set_xlabel('Input feature')\n",
    "    thisaxis.set_ylabel('Target value')\n",
    "    thisaxis.set_title('KNN Regression (K={})\\n\\\n",
    "Train $R^2 = {:.3f}$,  Test $R^2 = {:.3f}$'\n",
    "                      .format(K, train_score, test_score))\n",
    "    thisaxis.legend()\n",
    "    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we did for classification, let's look at the connection between model complexity and generalization ability as measured by the r-squared training and test values on the simple regression dataset. The series of plots on the notebook shows how the KNN regression algorithm fits the data for k = 1, 3, 7, 15, and in an extreme case of k = 55. It represents almost half the training points. We can see the same pattern in model complexity for k and N regression that we saw for k and N classification. Namely, that small values of k give models with higher complexity. And large values of k result in simpler models with lower complexity.\n",
    "\n",
    "Starting on the left when k = 1, the regression model fits the training data perfectly with a r-squared score of 1.0. But it's very bad at predicting the target values for new data samples, as reflected in the r-squared test score of only 0.155.\n",
    "\n",
    "As the value of k increases, again the training set score drops, but the model gets better at generalizing to new data and the test score goes up.\n",
    "\n",
    "Finally in this series, the model with k = 15 has the best test set performance, with an r-squared score of 0.485. Increasing k much further however to k = 55, results in both the training and test set scores dropping back down to lower levels, as the model now starts to under-fit. In other words, it's too simple to do well, even on the training data (Score for train sets is lower than for test sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression: Least Squares - linreg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_R1, y_R1, random_state=0)\n",
    "\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "print('linear model coeff (w): {}'.format(linreg.coef_.round(3)))\n",
    "print('linear model intercept(b): {:.3f}'.format(linreg.intercept_))\n",
    "print('R-Squared score (training): {:.3f}'.format(linreg.score(X_train, y_train)))\n",
    "print('R-Squared score (test): {:.3f}'.format(linreg.score(X_test, y_test)))\n",
    "b = float(linreg.coef_.round(3))\n",
    "a = linreg.intercept_.round(3)\n",
    "print('y = {}x + {}'.format(b,a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression: example plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "plt.scatter(X_R1, y_R1, marker='o', s=50, alpha=0.8)\n",
    "plt.plot(X_R1, linreg.coef_*X_R1 + linreg.intercept_,'r-',c='red', label='y = {}x + {}'.format(b,a))\n",
    "plt.title('Least-squares linear regression')\n",
    "plt.xlabel('Feature value (x)')\n",
    "plt.ylabel('Target value (y)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Captura2.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now on the Load_crime dataset **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime, random_state = 0)\n",
    "\n",
    "#linreg = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Models: Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression uses the same least-squares criterion, but with one difference. During the training phase, it adds a penalty for feature weights, the WI values that are too large as shown in the equation here. You'll see that large weights means mathematically that the sum of their squared values is large. Once ridge regression has estimated the WNB parameters for the linear model, the prediction of Y values for new instances is exactly the same as in least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "linridge = Ridge(alpha=20.0).fit(X_train, y_train)\n",
    "\n",
    "print('Crime dataset')\n",
    "print('ridge regression linear model intercept: {}'\n",
    "     .format(linridge.intercept_))\n",
    "print('ridge regression linear model coeff:\\n{}'\n",
    "     .format(linridge.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linridge.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linridge.score(X_test, y_test)))\n",
    "print('Number of non-zero features: {}'.format(np.sum(linridge.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*But, in order to get this model performing well, feature normalization will be needed, as it would scale all features transforming them to get a more fair estimator. The method to be used is MinMaxScaler(), fitting it to the trainer, which transforms, for any observation variable, the minimum value to 0 and the maximum to 1, doing it for all the variables of the dataset, having all the values of the observation in between 0 and 1. We would have to apply this scalar to the test set*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge regression with feature normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "linridge = Ridge(alpha=20.0).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('Crime Dataset Ridge Reg')\n",
    "print('Ridge regression linear model intercept: {}'.format(linridge.intercept_))\n",
    "print('Ridge regression linear model coeff: \\n {}'.format(linridge.coef_))\n",
    "print('R-Squared score on training set: {:.3f}'.format(linridge.score(X_train_scaled,y_train)))\n",
    "print('R-Squared score on test set: {:.3f}'.format(linridge.score(X_test_scaled, y_test)))\n",
    "print('Number of features taken in account: {}'.format(len(linridge.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, R-Squared is clearly better in Normalized Ridge Regression than in OLS Linear Regression (0.6 on test set against 0.5 in OLS and 0.5 in Regular Ridge Regression). So Ridge Regression improves results when it's NORMALIZED.\n",
    "\n",
    "Even though, regularization is not always that optimal. It is when the number of features is relatively large comparing to the size of the training dataset. Either if the size of the training dataset (number of rows/observations) is not that big or if there's not a huge number of features, regularization would not be that effective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ridge regression depending on regularization parameter: alpha**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ridge regression: effect of alpha regularization parameter\\n')\n",
    "for this_alpha in [0, 1, 10, 20, 50, 100, 1000]:\n",
    "    linridge = Ridge(alpha = this_alpha).fit(X_train_scaled, y_train)\n",
    "    r2_train = linridge.score(X_train_scaled, y_train)\n",
    "    r2_test = linridge.score(X_test_scaled, y_test)\n",
    "    num_coeff_bigger = np.sum(abs(linridge.coef_) > 1.0)\n",
    "    print('Alpha = {:.2f}\\nnum abs(coeff) > 1.0: {}, \\\n",
    "r-squared training: {:.2f}, r-squared test: {:.2f}\\n'\n",
    "         .format(this_alpha, num_coeff_bigger, r2_train, r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The best result is achieved with an alpha set in around 20, as r-squared test (0.60) is maximized*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized models: Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like ridge regression, lasso regression adds a regularisation penalty term to the ordinary least-squares objective, that causes the model W-coefficients to shrink towards zero. Lasso regression uses a slightly different regularisation term called an L1 penalty, instead of ridge regression's L2 penalty as shown here. The L1 penalty looks kind of similar to the L2 penalty, in that it computes a sum over the coefficients but it's some of the absolute values of the W-coefficients instead of a sum of squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso regression would be the better choice if the variables we have are not high in quantity (just a few) but are significant. Otherwise if there are lots of variables that contribute small or medium effects, ridge regression is typically the better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime, random_state=0)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "linlasso = Lasso(alpha=2.0, max_iter=10000).fit(X_train_scaled,y_train)\n",
    "linlasso_a = linlasso.intercept_\n",
    "linlasso_coef = linlasso.coef_\n",
    "linlasso_rsq_train = linlasso.score(X_train_scaled,y_train)\n",
    "linlasso_rsq_test = linlasso.score(X_test_scaled,y_test)\n",
    "\n",
    "print('CRIME DATASET')\n",
    "print('Lasso regression linear model intercept: {}'.format(linlasso_a.round(3)))\n",
    "print('Lasso regression coefficient:\\n{}'.format(linlasso.coef_))\n",
    "print('Features taken in account for Lasso Regression linear model: {}'.format(np.sum(linlasso_coef!=0)))\n",
    "print('R-Squared score for training set: {:.3f}'.format(linlasso_rsq_train))\n",
    "print('R-Squared score for test set: {:.3f}'.format(linlasso_rsq_test))\n",
    "print('\\n')\n",
    "print('Features with non-zero weight (sorted by absolute magnitude):')\n",
    "for e in sorted(list(zip(list(X_crime), linlasso.coef_)),key = lambda e: -abs(e[1])):\n",
    "    if e[1]!=0:\n",
    "        print('\\t{}: {:.3f}'.format(e[0],e[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lasso regression with regularization parameter: ALPHA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lasso regression: effect of alpha regularization parameter on number of features kept in final model\\n')\n",
    "\n",
    "for alpha in [0.5, 1, 2, 3, 5, 10, 20, 50]:\n",
    "    linlasso = Lasso(alpha, max_iter=10000).fit(X_train_scaled,y_train)\n",
    "    r2_train = linlasso.score(X_train_scaled,y_train)\n",
    "    r2_test = linlasso.score(X_test_scaled,y_test)\n",
    "    \n",
    "    print('Alpha = {:.2f}\\nIntercept: {:.3f}\\nFeatures kept: {}\\nR-squared Training Set: {:.3f}\\nR-squared Test set: {:.3f}\\n'\n",
    "         .format(alpha, linlasso.intercept_,np.sum(linlasso.coef_!=0),r2_train,r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The best result is achieved with an alpha set in around 3, as r-squared test (0.63) is maximized. Just 17 variables will be non-zero ones*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized models: Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Captura3.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we add these new polynomial features, we're essentially adding to the model's ability to capture interactions between the different variables by adding them as features to the linear model. For example, it may be that housing prices vary as a quadratic function of both the lat size that a house sits on, and the amount of taxes paid on the property as a theoretical example. A simple linear model could not capture this nonlinear relationship, but by adding nonlinear features like polynomials to the linear regression model, we can capture this nonlinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_F1, y_F1,random_state = 0)\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('linear model coeff (w): {}'\n",
    "     .format(linreg.coef_))\n",
    "print('linear model intercept (b): {:.3f}'\n",
    "     .format(linreg.intercept_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))\n",
    "\n",
    "print('\\nNow we transform the original input data to add\\n\\\n",
    "polynomial features up to degree 2 (quadratic)\\n')\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_F1_poly = poly.fit_transform(X_F1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y_F1,\n",
    "                                                   random_state = 0)\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('(poly deg 2) linear model coeff (w):\\n{}'\n",
    "     .format(linreg.coef_))\n",
    "print('(poly deg 2) linear model intercept (b): {:.3f}'\n",
    "     .format(linreg.intercept_))\n",
    "print('(poly deg 2) R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('(poly deg 2) R-squared score (test): {:.3f}\\n'\n",
    "     .format(linreg.score(X_test, y_test)))\n",
    "\n",
    "print('\\nAddition of many polynomial features often leads to\\n\\\n",
    "overfitting, so we often use polynomial features in combination\\n\\\n",
    "with regression that has a regularization penalty, like ridge\\n\\\n",
    "regression. The result on test model (R2) seems to be better comparing to LinearReg\\n')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y_F1,\n",
    "                                                   random_state = 0)\n",
    "linreg = Ridge().fit(X_train, y_train)\n",
    "\n",
    "print('(poly deg 2 + ridge) linear model coeff (w):\\n{}'\n",
    "     .format(linreg.coef_))\n",
    "print('(poly deg 2 + ridge) linear model intercept (b): {:.3f}'\n",
    "     .format(linreg.intercept_))\n",
    "print('(poly deg 2 + ridge) R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('(poly deg 2 + ridge) R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear models for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression for binary classification on fruits dataset using height, width features (positive class: apple, negative class: others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from adspy_shared_utilities import (\n",
    "plot_class_regions_for_classifier_subplot)\n",
    "\n",
    "X_fruits_2d = X_fruits[['height','width']]\n",
    "y_fruits_2d = fruits['fruit_label']\n",
    "\n",
    "fig, subaxes = plt.subplots(1, 1, figsize=(7, 5))\n",
    "y_fruits_apple = y_fruits_2d == 1   # make into a binary problem: apples vs everything else\n",
    "X_train, X_test, y_train, y_test = (\n",
    "train_test_split(X_fruits_2d.as_matrix(),\n",
    "                y_fruits_apple.as_matrix(),\n",
    "                random_state = 0))\n",
    "\n",
    "clf = LogisticRegression(C=100).fit(X_train, y_train) # parameter C controls amount of regularization\n",
    "plot_class_regions_for_classifier_subplot(clf, X_train, y_train, None,\n",
    "                                         None, 'Logistic regression \\\n",
    "for binary classification\\nFruit dataset: Apple vs others',\n",
    "                                         subaxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 6\n",
    "w = 8\n",
    "print('A fruit with height {} and width {} is predicted to be: {}'\n",
    ".format(h,w, ['not an apple', 'an apple'][clf.predict([[h,w]])[0]]))\n",
    "\n",
    "h = 10\n",
    "w = 7\n",
    "print('A fruit with height {} and width {} is predicted to be: {}'\n",
    "     .format(h,w, ['not an apple', 'an apple'][clf.predict([[h,w]])[0]]))\n",
    "subaxes.set_xlabel('height')\n",
    "subaxes.set_ylabel('width')\n",
    "print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression on simple synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from adspy_shared_utilities import (plot_class_regions_for_classifier_subplot)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "fig, subaxes = plt.subplots(1, 1, figsize=(7, 5))\n",
    "clf = LogisticRegression().fit(X_train, y_train)\n",
    "title = 'Logistic regression, simple synthetic dataset C = {:.3f}'.format(1.0)\n",
    "plot_class_regions_for_classifier_subplot(clf, X_train, y_train,\n",
    "                                         None, None, title, subaxes)\n",
    "\n",
    "print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application to real dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state = 0)\n",
    "\n",
    "clf = LogisticRegression().fit(X_train, y_train)\n",
    "print('Breast cancer dataset')\n",
    "print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support vector Machines - Binary class classification method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sample binary classification problem with TWO features*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from adspy_shared_utilities import plot_class_regions_for_classifier_subplot\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2, random_state =0)\n",
    "\n",
    "fig,subaxes = plt.subplots(1,1, figsize=(7,5))\n",
    "cc = 1 # this parameter controls the margin btw classes. It is the key of this kind of models\n",
    "clf = SVC(kernel = 'linear', C=cc).fit(X_train, y_train)\n",
    "print(clf)\n",
    "title='Linear SVC, c={:.3f}'.format(cc)\n",
    "plot_class_regions_for_classifier_subplot(clf,X_train, y_train, None, None,title , subaxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Support Vector Machine: Figuring out the C parameter**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Larger values of C represent less regularization and will cause the model to fit the training set with these few errors as possible, even if it means using a small immersion decision boundary. Very small values of C on the other hand use more regularization that encourages the classifier to find a large marge on decision boundary, even if that decision boundary leads to more points being misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from adspy_shared_utilities import plot_class_regions_for_classifier_subplot\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2, random_state = 0)\n",
    "fig,subaxes = plt.subplots(1,2,figsize=(8,4))\n",
    "\n",
    "for c,subplot in zip([0.00001, 100],subaxes):\n",
    "    clf = LinearSVC(C=c).fit(X_train, y_train)\n",
    "    title = 'Linear SVC, C = {:.5f}'.format(c)\n",
    "    plot_class_regions_for_classifier_subplot(clf, X_train, y_train,\n",
    "                                             None, None, title, subplot)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Application to a real world dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state =0)\n",
    "cc=10\n",
    "clf = LinearSVC(C=cc,random_state=0).fit(X_train, y_train)\n",
    "print('Breast Cancer Dataset')\n",
    "a = clf.intercept_\n",
    "b = clf.coef_\n",
    "print('Intercept = {}'.format(a))\n",
    "print('Coefficient = {}'.format(b))\n",
    "print('Accuracy of Linear SVC classifier on training set: {:.2f}'.format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of Linear SVC classifier on test set: {:.2f}'.format(clf.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class classification with linear models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear SVC with M classes generates M one vs rest classifiers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_fruits_2d, y_fruits_2d, random_state = 0)\n",
    "\n",
    "clf = LinearSVC(C=5, random_state=67).fit(X_train,y_train)\n",
    "print('Coefficients:\\n',clf.coef_)\n",
    "print('Intercepts:\\n',clf.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*There is one intercept for each class to be classified. There is a pair of features for each class, so there's two coefficients for each intercept(class)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_names_fruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n in range(0,4):\n",
    "    print([target_names_fruits[n]])\n",
    "    print('y = {:.3f}{} + {:.3f}{} + {:.3f}\\n'\n",
    "          .format(clf.coef_[n][0],X_fruits_2d.columns[0],clf.coef_[n][1],X_fruits_2d.columns[1],clf.intercept_[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names_fruits[clf.predict([[2,6]])[0]-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applying it to the fruit dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "colors = ['r', 'g', 'b', 'y']\n",
    "cmap_fruits = ListedColormap(['#FF0000', '#00FF00', '#0000FF','#FFFF00'])\n",
    "\n",
    "plt.scatter(X_fruits_2d[['height']], X_fruits_2d[['width']],\n",
    "           c=y_fruits_2d, cmap=cmap_fruits, edgecolor = 'black', alpha=0.7)\n",
    "\n",
    "x_0_range = np.linspace(-10, 15)\n",
    "\n",
    "for w, b, color in zip(clf.coef_, clf.intercept_, ['r', 'g', 'b', 'y']):\n",
    "    # Since class prediction with a linear model uses the formula y = w_0 x_0 + w_1 x_1 + b, \n",
    "    # and the decision boundary is defined as being all points with y = 0, to plot x_1 as a \n",
    "    # function of x_0 we just solve w_0 x_0 + w_1 x_1 + b = 0 for x_1:\n",
    "    plt.plot(x_0_range, -(x_0_range * w[0] + b) / w[1], c=color, alpha=.8)\n",
    "    \n",
    "plt.legend(target_names_fruits)\n",
    "plt.xlabel('height')\n",
    "plt.ylabel('width')\n",
    "plt.xlim(-2, 12)\n",
    "plt.ylim(-2, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernelized Support Vector Machines - Complex binary classification problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we gave the linear support vector machine a harder problem, where the classes are no longer linearly separable. A simple linear decision boundary would not be enough to determine the classification of all these points in a proper way. This method transforms data space from 2-dimensional space to a 3-dimensional space (similar to the polynomial regression method previoulsy treated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Captura4.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Captura6.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RBF Kernel\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from adspy_shared_utilities import plot_class_regions_for_classifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state = 0)\n",
    "clf = SVC().fit(X_train, y_train) # by defect, the kernel is rbf and the gamma is auto\n",
    "print(clf)\n",
    "print('Accuracy on training set: {:.3f}'.format(clf.score(X_train,y_train)))\n",
    "print('Accuracy on test set: {:.3f}'.format(clf.score(X_test,y_test)))\n",
    "print(clf.intercept_)\n",
    "plot_class_regions_for_classifier(clf, X_train, y_train, None, None, 'Support Vector Classifier: RBF Kernel\\n On training set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = SVC(gamma=0.01).fit(X_train, y_train)\n",
    "plot_class_regions_for_classifier(clf, X_train, y_train, None, None, 'Support Vector Classifier: RBF Kernel\\n On training set. Gamma = 0.01')\n",
    "clf = SVC(gamma=1.00).fit(X_train, y_train)\n",
    "plot_class_regions_for_classifier(clf, X_train, y_train, None, None, 'Support Vector Classifier: RBF Kernel\\n On training set. Gamma = 1')\n",
    "clf = SVC(gamma=10.00).fit(X_train, y_train)\n",
    "plot_class_regions_for_classifier(clf, X_train, y_train, None, None, 'Support Vector Classifier: RBF Kernel\\n On training set. Gamma = 10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The parameter Gamma controls how far the influence of a single trending example reaches, which in turn affects how tightly the decision boundaries end up surrounding points in the input space.*\n",
    "\n",
    "*The larger the gamma, the smaller the influence of every point would be, needing more proximity of similar points for creating a particular feature area. So higher gamma means more complexity in the model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Polynomial Kernel\n",
    "clf2 = SVC(kernel='poly',degree=3).fit(X_train, y_train)\n",
    "print(clf2.intercept_)\n",
    "print(clf2)\n",
    "print('Accuracy on training set: {:.3f}'.format(clf2.score(X_train,y_train)))\n",
    "print('Accuracy on test set: {:.3f}'.format(clf2.score(X_test,y_test)))\n",
    "plot_class_regions_for_classifier(clf2, X_train, y_train, None, None, 'Support Vector Classifier: Polynomial kernel, degree=3\\n On training set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The polynomial kernel takes additional parameter **degree** that controls the model complexity and the computational cost of this transformation.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine with RBF kernel: gamma parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adspy_shared_utilities import plot_class_regions_for_classifier_subplot\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state = 0)\n",
    "fig, subaxes = plt.subplots(3, 1, figsize=(4, 11))\n",
    "\n",
    "for this_gamma, subplot in zip([0.01, 1.0, 10.0], subaxes):\n",
    "    clf = SVC(kernel = 'rbf', gamma=this_gamma).fit(X_train, y_train)\n",
    "    title = 'Support Vector Classifier: \\nRBF kernel, gamma = {:.2f}'.format(this_gamma)\n",
    "    plot_class_regions_for_classifier_subplot(clf, X_train, y_train,\n",
    "                                             None, None, title, subplot)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine with RBF kernel: using both C and gamma parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adspy_shared_utilities import plot_class_regions_for_classifier_subplot\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state = 0)\n",
    "fig, subaxes = plt.subplots(3, 4, figsize=(15, 10), dpi=50)\n",
    "\n",
    "for this_gamma, this_axis in zip([0.01,1,5], subaxes):\n",
    "    for this_C,subplot in zip([0.1,1,15,250],this_axis):\n",
    "        title = 'gamma = {:.2f}, C = {:.2f}'.format(this_gamma, this_C)\n",
    "        clf = SVC(kernel = 'rbf', gamma = this_gamma, C = this_C).fit(X_train, y_train)\n",
    "        plot_class_regions_for_classifier_subplot(clf, X_train, y_train, X_test, y_test, title, subplot)\n",
    "        \n",
    "        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application of SVMs to a real dataset: unnormalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "clf = SVC(C=10).fit(X_train, y_train)\n",
    "print('Breast cancer dataset (unnormalized features)')\n",
    "print('Accuracy of RBF-kernel SVC on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of RBF-kernel SVC on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The model is clearly **overfitting**, as the test set score is way far below the training one*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application of SVMs to a real dataset: normalized data with MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "clf = SVC(C=10).fit(X_train_scaled, y_train)\n",
    "print('Breast cancer dataset (normalized with MinMax scaling)')\n",
    "print('RBF-kernel SVC (with MinMax scaling) training set accuracy: {:.2f}'.format(clf.score(X_train_scaled,y_train)))\n",
    "print('RBF-kernel SVC (with MinMax scaling) test set accuracy: {:.2f}'.format(clf.score(X_test_scaled,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=5)\n",
    "X = X_fruits_2d.as_matrix()\n",
    "y = y_fruits_2d.as_matrix()\n",
    "\n",
    "cv_scores = cross_val_score(clf,X,y)\n",
    "print('Cross-validation scores (3-fold):', cv_scores)\n",
    "print('Mean cross-validation score (3-fold): {:.3f}'\n",
    "     .format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of validation curve in a SVM Classifier Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Validation curves show sensivity of the score of the model to changes in an important parameter (ex. gamma)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "param_range = np.logspace(-3,3,4)\n",
    "X = X_fruits_2d.as_matrix()\n",
    "y = y_fruits_2d.as_matrix()\n",
    "train_scores, test_scores = validation_curve(SVC(), X, y, param_name = 'gamma', param_range=param_range, cv=3)\n",
    "print('Train Scores\\n',train_scores,'\\n')\n",
    "print('Test Scores\\n',test_scores)\n",
    "cvscore_test = np.mean(test_scores)\n",
    "cvscore_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.title('Validation Curve with SVM')\n",
    "plt.xlabel('$\\gamma$ (gamma)')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0.0, 1.1)\n",
    "lw = 2\n",
    "\n",
    "plt.semilogx(param_range, train_scores_mean, label='Training score',\n",
    "            color='darkorange', lw=lw)\n",
    "\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                color='darkorange', lw=lw)\n",
    "\n",
    "plt.semilogx(param_range, test_scores_mean, label='Cross-validation score',\n",
    "            color='navy', lw=lw)\n",
    "\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                color='navy', lw=lw)\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The goal when building a decision tree is to find the sequence of questions that has the best accuracy at classifying the data in the fewest steps*\n",
    "\n",
    "Let's go through this process working on the dataset **iris** - sklearn.datasets(load_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from adspy_shared_utilities import plot_decision_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)\n",
    "clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy of the decision tree classifier on training set: {:.3f}'.format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of the decision tree classifier on test set: {:.3f}'.format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This kind of models are likely to overfitting, in general, particularly as long as there's not a strategy behind its design**. In order to avoid this, the technique to be used is called PREPRUNING. It consists on limiting the number of split deciders done in the whole tree via the parameter max_depth. It allows the model to allow being so deep so it would generalize better (which leads to a better balance btw test score and train score, or a higher test accuracy than training one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting max decision tree depth to help avoid overfitting - Modifying the max_depth parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = DecisionTreeClassifier(max_depth=3).fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy of the decision tree classifier on training set: {:.3f}'.format(clf2.score(X_train, y_train)))\n",
    "print('Accuracy of the decision tree classifier on test set: {:.3f}'.format(clf2.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing decision trees**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_tree(clf,iris.feature_names,iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_tree(clf2,iris.feature_names,iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance. How important is a feature to overall prediction accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A feature importance of 0 means that the feature is not used in the prediction. A value of 1 means that the feature predicts the target (Y) perfectly. The analysis normalizes each feature's ratio in order to get them summing 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adspy_shared_utilities import plot_feature_importances\n",
    "\n",
    "plt.figure(figsize=(10,4),dpi=80)\n",
    "plot_feature_importances(clf,iris.feature_names)\n",
    "\n",
    "print('Feature importances: {}'.format(clf.feature_importances_))\n",
    "print(iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adspy_shared_utilities import plot_class_regions_for_classifier_subplot\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state = 0)\n",
    "fig, subaxes = plt.subplots(6, 1, figsize=(6, 32))\n",
    "\n",
    "pair_list = [[0,1], [0,2], [0,3], [1,2], [1,3], [2,3]]\n",
    "tree_max_depth = 4\n",
    "\n",
    "for pair, axis in zip(pair_list, subaxes):\n",
    "    X = X_train[:, pair]\n",
    "    y = y_train\n",
    "    \n",
    "    clf = DecisionTreeClassifier(max_depth=tree_max_depth).fit(X, y)\n",
    "    title = 'Decision Tree, max_depth = {:d}'.format(tree_max_depth)\n",
    "    plot_class_regions_for_classifier_subplot(clf, X, y, None,\n",
    "                                             None, title, axis,\n",
    "                                             iris.target_names)\n",
    "    \n",
    "    axis.set_xlabel(iris.feature_names[pair[0]])\n",
    "    axis.set_ylabel(iris.feature_names[pair[1]])\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees on a real-world-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from adspy_shared_utilities import plot_decision_tree\n",
    "from adspy_shared_utilities import plot_feature_importances\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_cancer,y_cancer,random_state=0)\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=4,min_samples_leaf=8,random_state=0).fit(X_train,y_train)\n",
    "\n",
    "plot_decision_tree(clf,cancer.feature_names,cancer.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Breast Cancer Decision Tree')\n",
    "print('Accuracy of DT classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of DT classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))\n",
    "\n",
    "print(clf.feature_importances_)\n",
    "\n",
    "plt.figure(figsize=(10,6),dpi=80)\n",
    "plot_feature_importances(clf, cancer.feature_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"Captura7.PNG\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
